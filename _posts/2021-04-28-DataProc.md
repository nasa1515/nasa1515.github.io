---
layout: post
title: "[DATA] - GCP DataProc spark Cluster로 ETL 후 BigQuery에 적재"
author: nasa1515
categories: DATA
date: 2021-04-28 12:36
comments: true
cover: "/assets/1800-550.jpg"
tags: DATA
---



## **GCP DataProc spark Cluster로 ETL 후 BigQuery에 적재**


<br/>

**머리말**  

**이번에는 DataProc(Hadoop/Spark)를 사용하여** 
**대용량의 데이터를 처리하는 방법에 대해서 다룹니다.**  
**물론 파이썬을 첨가해서**  


---

**DATA 시리즈**




**이론**



 - [Apache Spark](https://nasa1515.github.io/data/2021/03/03/spark.html)


**실습** 

 - [Azure Synapse Analytics](https://nasa1515.github.io/data/2021/02/25/azure-synapse.html)
 - [Azure VM에 Apache Spark v3.0 Standalone 설치 With Zeppelin](https://nasa1515.github.io/data/2021/03/04/Spark2.html)
 - [Hadoop 3.3.0 Full Distribute mode infra 구축](https://nasa1515.github.io/data/2021/03/08/hadoop.html)
 - [Apache Spark v3.0 on yarn 설치 With Zeppelin](https://nasa1515.github.io/data/2021/03/10/spark-yarn.html)

---

**목차**

- [DataProc에 대해서..](#a1)
- [DataProc Cluster 생성](#a2)
- [Data 준비하기](#a3)
- [실행 결과](#a4)


--- 

## **DataProc에 대해서..** <a name="a1"></a> 

*Dataproc은 일괄 처리, 쿼리, 스트리밍, 머신 러닝에 오픈소스 데이터 도구를 활용할 수 있는 관리형 Spark 및 Hadoop 서비스입니다.*  
**즉 지금까지 귀찮게 Spark, Hadoop을 연동하는 과정을 없애고 사용만하면 되는 서비스라고 볼 수 있습니다.**  

**여기서 DataFlow와 DataProc의 차이에 대해서 궁금증이 생겼는데**  
**두 툴 모두 ETL을 하는 툴에 대해서는 공통점을 가지고 있지만**  
**DataFlow는 Serverless 서비스로 Streaming, Batch Flow를 Code로 관리하고 싶으면 사용하고**  
**DataProc은 기존에 HDFS 같은 Hadoop EcoSystem에 종속되어 있는 시스템을 가지고 있다면 사용하기 좋다고 합니다.**


<br/>

## **DataProc Cluster 생성** <a name="a2"></a> 


* **GCP 탐색 메뉴 > Dataproc > 클러스터 선택 > 클러스터 만들기**

    ![다운로드](https://user-images.githubusercontent.com/69498804/116354676-b8134080-a833-11eb-8b5a-249126ff2798.png)


<br/>

* **클러스터 필드 설정 (이름을 제외한 나머지 부분은 기본값)**

    ![캡처](https://user-images.githubusercontent.com/69498804/116355090-4f789380-a834-11eb-8880-311c70b982b2.JPG)

    <br/>

* **프로비저닝 과정을 3분정도 거치고 다음과 같이 생성이 완료됩니다.**

    ![2](https://user-images.githubusercontent.com/69498804/116356531-5b655500-a836-11eb-873e-f5701fbc4d9a.JPG)

<br/>

## **Data 준비하기** <a name="a3"></a> 

**이번 포스트에서는 BigQuery에서 공개적으로 제공하는 DataSet을 이용합니다.**  

**Dataproc Cluster는 GCS Connector를 기본으로 제공하여**  
**다른 설정없이 GCS에 있는 데이터에 바로 액세스가 가능합니다.**  
**저는 이를 이용해서 BigQuery의 공개 DataSet의 특정 테이블을 Cloud Storage로 Export하여**  
**Export한 데이터에 바로 접근하여 사용, 퍼포먼스 테스트를 해보겠습니다.**  

<br/>


* **데이터 정보**  

    ![캡처3](https://user-images.githubusercontent.com/69498804/116357157-24437380-a837-11eb-9047-048e8e5a018d.JPG)

    * **Table ID : bigquery-public-data:covid19_weathersource_com.postal_code_day_history**
    * **Table 크기 : 약 300** 

    <br/>

* **데이터 형식** 

    ![캡처4](https://user-images.githubusercontent.com/69498804/116357277-476e2300-a837-11eb-920c-682f3e9dfa19.JPG)

    **데이터의 내용은 나라 별 COVID-19의 기상상태 데이터입니다.**  

<br/>

* **Cloude Storage 생성 (GCS) - 쿼리 결과 데이러(CSV)틑 쌓는 곳**  

    ![캡처5](https://user-images.githubusercontent.com/69498804/116358291-720cab80-a838-11eb-8d31-a13169891652.JPG)

    * **Region을 Bigquery와 맞춰주어야 합니다.** 

<br/>

* **BigQuery DataSet, Table 생성 (쿼리 결과 데이터를 쌓는 곳)**

    ![캡처6](https://user-images.githubusercontent.com/69498804/116358650-d7609c80-a838-11eb-8791-4acddef9cb46.JPG)

<br/>


* **저는 Python으로 간단한 코드를 작성해서 다음과 같이 데이터를 분류했습니다.**  

    **공개 DataSet에서 쿼리 결과를 다른 테이블에 저장하는 코드** 

    ```
    from google.cloud import bigquery

    # Construct a BigQuery client object.
    client = bigquery.Client()

    # TODO(developer): Set table_id to the ID of the destination table.
    table_id = "lws-cloocus.ustest.ustable"

    job_config = bigquery.QueryJobConfig(destination=table_id)

    sql = 'SELECT * FROM `bigquery-public-data.covid19_weathersource_com.postal_code_day_history` LIMIT 34230421'

    # Start the query, passing in the extra configuration.
    query_job = client.query(sql, job_config=job_config)  # Make an API request.
    query_job.result()  # Wait for the job to complete.

    print("Query results loaded to the table {}".format(table_id))
    ```
    * **아셔야 하는 건 Table을 복사하는 것과 데이터만(쿼리결과)복사하는 것은 다릅니다.**  
    **Table을 그대로 복사하게되면 Table의 정보까지 저장됩니다..**


<br/>

* **해당 Code를 실행시키게 되면 다음과 같이 특정 Table에 쿼리결과가 저장됩니다.**  


    ![캡처](https://user-images.githubusercontent.com/69498804/116361301-c9f8e180-a83b-11eb-90fb-a61cfbbd8fc9.JPG)

<br>

* **쿼리결과가 저장되어있는 Table의 데이터를 csv로 변환해서 GCS로 저장**  

    ```
    # Source option
    project = "lws-cloocus"
    dataset_id = "ustest"
    table_id = "ustable"


    # 용량 많은 Table (1G이상)은 * 정규표현식으로 Table 읽어서 csv화 시켜야 함.
    destination_uri = "gs://{}/{}".format(bucket_name, "result*.csv")
    dataset_ref = bigquery.DatasetReference(project, dataset_id)
    table_ref = dataset_ref.table(table_id)

    extract_job = client.extract_table(
        table_ref,
        destination_uri,
        # Location must match that of the source table.
        location="US",
    )  # API request
    extract_job.result()  # Waits for job to complete.

    print(
        "Exported {}:{}.{} to {}".format(project, dataset_id, table_id, destination_uri)
    )
    ```

    * **BigQeury에서 Data를 export 할 때 한번에 1GB 단위까지 밖에 지원되지 않습니다.**  
    **때문에 * 와일드카드를 사용해서 CSV File을 분리해줘야 합니다.**  

    <br/>

* **해당 코드를 실행시키면 다음과 같이 GCS에 Data가 저장됩니다.**  


    ![캡처3](https://user-images.githubusercontent.com/69498804/116361977-8783d480-a83c-11eb-8d1e-5d447ce71323.JPG)

    * **다음과 같이 용량이 일정하게 나눠서 저장됩니다.**  

<br/>

* **저장된 CSV File을 Local로 다운받아서 NotePad로 확인해보죠**  

    ![23](https://user-images.githubusercontent.com/69498804/116362297-ddf11300-a83c-11eb-8944-4694fb94bd4e.JPG)

    * **다음과 같이 맨 윗줄은 헤더 정보, 나머지는 데이터 값만 저장됩니다.**  

<br/>

