---
layout: post
title: "[DATA] - Hadoop Full Distribute mode infra 구축"
author: nasa1515
categories: DATA
date: 2021-03-08 12:36
comments: true
cover: "/assets/1800-550.jpg"
tags: DATA
---



## **Hadoop Full Distribute mode infra 구축**


<br/>

**머리말**  

**앞이 막막합니다.**  
**저번 포스트에서 이미 인프라 구성을 끝냈어야 했는데...**  
**이번 포스트에서라도 마무리 지어보죠**  








  


 
---

**DATA 시리즈**





* **이론**


---



**목차**


- [설치 환경](#a1)
- [Hadoop 설치 전 사전 작업](#a2)
- [Hadoop 설치 및 연동](#a3)


--- 

## **설치 환경**    <a name="a1"></a> 

* #### **Hadoop (Full-Distribute Mode)**

    |Server|Master|Worker1|Worker2|
    |---|:---:|:---:|:---:|
    |OS|CentOS 8.2|CentOS 8.2|CentOS 8.2|
    |Disk|30G|30G|30G|
    |MEM|14G|14G|14G|
    |CPU|4.Core|4.Core|4.Core|

<br/>

* **VM (Azure)**

    * **Hadoop Master IP : 10.0.0.5** 
    * **Hadoop Worker1 IP : 10.0.0.6** 
    * **Hadoop Worker2 IP : 10.0.0.7** 

<br/>

## **Hadoop 설치 전 사전 작업**    <a name="a1"></a> 


* #### **각 서버별 HOSTNAME 설정 및 hosts 설정 [전 서버 동일]**  

    ```
    [root@hadoop-master ~]# cat /etc/hosts
    10.0.0.5        hadoop-master
    10.0.0.6        hadoop-worker
    10.0.0.7        hadoop-worker2
    ```

<br/>

* #### **hadoop 사용자 생성**

    ```
    [root@hadoop-master ~]# useradd -m hadoop
    [root@hadoop-master ~]# passwd hadoop
    Changing password for user hadoop.
    New password:
    BAD PASSWORD: The password is shorter than 8 characters
    Retype new password:
    passwd: all authentication tokens updated successfully.
    [root@hadoop-master ~]# usermod -G wheel hadoop
    ```

<br/>

* #### **SSH 설정 [hadoop 계정에서]**  

* #### **Master <-> Worker 간의 ssh 설정을 위해 다음 작업을 진행한다.**  

    ```
    [hadoop@hadoop-master ~]$ ssh-keygen -t rsa
    Generating public/private rsa key pair.
    Enter file in which to save the key (/home/hadoop/.ssh/id_rsa):
    Created directory '/home/hadoop/.ssh'.
    Enter passphrase (empty for no passphrase):
    Enter same passphrase again:
    Your identification has been saved in /home/hadoop/.ssh/id_rsa.
    Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.
    ```

    **RHA 공개키는 사용자 계정의 홈 디렉터리에 있는 .ssh 폴더에 생성됩니다.**  
    **생성된 공개키를 ssh-copy-id 명령으로 Worker 서버에 복사합니다.** 

    ```
    [hadoop@hadoop-master ~]$ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@hadoop-worker
    /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/hadoop/.ssh/id_rsa.pub"
    The authenticity of host 'hadoop-worker (10.0.0.6)' can't be established.
    ECDSA key fingerprint is SHA256:w8Q+47MDyqSgD9cvqRxN91gu/tn/BDrXIa/y26Dst/Y.
    Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
    /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
    /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
    hadoop@hadoop-worker's password:

    Number of key(s) added: 1

    Now try logging into the machine, with:   "ssh 'hadoop@hadoop-worker'"
    and check to make sure that only the key(s) you wanted were added.

    [hadoop@hadoop-master ~]$ ssh hadoop-worker
    Activate the web console with: systemctl enable --now cockpit.socket

    [hadoop@hadoop-worker ~]$ exit
    logout
    Connection to hadoop-worker closed.
    ```
    **그럼 다음과 같이 암호입력 없이 ssh 접속이 가능합니다.**   
    **동일하게 Worker2에도 진행합니다.**  

<br/>


* #### **JAVA 설치 [root 계정으로]**  

    ```
    [root@hadoop-master ~]# yum install -y java-1.8.0-openjdk-devel.x86_64
    [root@hadoop-master ~]#
    [root@hadoop-master ~]# javac -version
    javac 1.8.0_275
    ```

<br/>

---

## **Hadoop 설치 [Hadoop 계정으로]**    <a name="a2"></a> 

* **미러 사이트 : apache.mirror.cdnetworks.com/hadoop/common**
* **Version : 3.3.0 / 추후에 HBase를 설치하려면 3.1.1 Version 이상을 사용해야함.**  

* **Download**
    * **Download 서버 : master**
    * **Download 위치 : /usr/local**


    ```
    cd /usr/local; \
    sudo wget http://apache.mirror.cdnetworks.com/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz; \
    sudo tar xzvf hadoop-3.3.0.tar.gz; \
    sudo rm -rf hadoop-3.3.0.tar.gz; \
    sudo mv hadoop-3.3.0 hadoop
    [hadoop@hadoop-master local]$ ls
    bin  etc  games  hadoop  include  lib  lib64  libexec  sbin  share  src
    ```

<br/>

* #### **환경 변수 설정 [전체 서버]** 

    **지정해준 JAVA_HOME, HADOOP_HOME을 PATH로 묶어 사용합니다.**  
    ```
    [root@hadoop-master ~]# vim /etc/profile

    export JAVA_HOME="/usr/bin/jvm/java-1.8.0-openjdk-1.8.0.275.b01-1.el8_3.x86_64"
    export HADOOP_HOME="/usr/local/hadoop"
    export PATH="$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:"

    
    [root@hadoop-master ~]# source /etc/profile
    [root@hadoop-master ~]#
    [root@hadoop-master ~]# echo $JAVA_HOME
    /usr/bin/jvm/java-1.8.0-openjdk-1.8.0.275.b01-1.el8_3.x86_64
    [root@hadoop-master ~]# echo $HADOOP_HOME
    /usr/local/hadoop
    [root@hadoop-master ~]# echo $PATH
    /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/usr/bin/jvm/java-1.8.0-openjdk-1.8.0.275.b01-1.el8_3.x86_64/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin:
    ```

<br/>

---


## **Hadoop 구성 [Hadoop 계정으로]**    <a name="a3"></a> 

* **진행 노드 : master**
* **파일 위치 : /usr/local/hadoop/etc/hadoop**

* **설정 파일 목록** 

    * **core.site.xml : HDFS, Map Reduce 환경 정보** 
    * **hdfs.site.xml : HDFS 환경 정보**
    * **yarn-site.xml : yarn 환경 정보**
    * **mapred-site.xml : Map Reduce 환경 정보**
    * **hadoop-env.sh : Hadoop 실행 시 필요한 shell script 환경 변수**  
    

<br/>

* #### **core.site.xml**

    ```
    <configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop-master:9000</value>
    </property>
    </configuration>
    ```

<br/>

* #### **hdfs.site.xml**

    ```
    <configuration>   
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/usr/local/hadoop/namenode</value>
    </property>   
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/usr/local/hadoop/datanode</value>
    </property> 
    </configuration>
    ```

<br/>

* #### **yarn-site.xml**

    ```
    <configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property> 
    </configuration>
    ```

<br/>

* #### **mapred-siter.xml**

    ```
    <configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    </configuration>
    ```

<br/>

* #### **hadoop-env.sh** 

    **JAVA_HOME의 경우 필수적이나 나머지는 선택사항입니다.** 


    ```
    export JAVA_HOME="/usr/bin/jvm/java-1.8.0-openjdk-1.8.0.275.b01-1.el8_3.x86_64"
    export HADOOP_HOME="/usr/local/hadoop"
    export HADOOP_CONF_DIR="$HADOOP_HOME/etc/hadoop"
    export HADOOP_LOG_DIR="$HADOOP_HOME/logs"
    export HADOOP_PID_DIR="$HADOOP_HOME/pids"
    ```

<br/>


* #### **Hadoop forder 생성** 

    * **진행 노드 : master**
    * **위치 : /usr/local/hadoop**

    ```
    [root@hadoop-master hadoop]# cd /usr/local/hadoop; \
    > mkdir namenode; \
    > mkdir datanode
    ```

<br/>


* #### **Hadoop 배포** 

    **master에서 worker1,2에 전송합니다.**

    ```
    # worker 1로 전송
    scp -r /usr/local/hadoop hadoop-worker:/usr/local

# slave2 /usr/local로 전송
scp -r /usr/local/hadoop root@192.168.137.132:/usr/local