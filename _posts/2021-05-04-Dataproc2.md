---
layout: post
title: "[DATA] - GCP DataProc 2탄 Pyspark JOB Access"
author: nasa1515
categories: DATA
date: 2021-04-28 12:36
comments: true
cover: "/assets/1800-550.jpg"
tags: DATA
---



## **GCP DataProc 2탄 Pyspark JOB Access**


<br/>

**머리말**  

**저번 포스트에서 DataProc에 대한 설명과 간단한 사용법을 다뤄봤었습니다.**  
**이번에는 DataProc Cluster에 Pyspark Script를 사용해서** 
**자동화 JOB을 만들어 보겠습니다.**  
**역시 파이썬을 첨가해서**  


---

**DATA 시리즈**




**이론**



 - [Apache Spark](https://nasa1515.github.io/data/2021/03/03/spark.html)


**실습** 

 - [Azure Synapse Analytics](https://nasa1515.github.io/data/2021/02/25/azure-synapse.html)
 - [Azure VM에 Apache Spark v3.0 Standalone 설치 With Zeppelin](https://nasa1515.github.io/data/2021/03/04/Spark2.html)
 - [Hadoop 3.3.0 Full Distribute mode infra 구축](https://nasa1515.github.io/data/2021/03/08/hadoop.html)
 - [Apache Spark v3.0 on yarn 설치 With Zeppelin](https://nasa1515.github.io/data/2021/03/10/spark-yarn.html)

---

**목차**

- [Data](#a1)
- [Python Script](#a2)
- [Data 준비하기](#a3)
- [실행 결과](#a4)
- [Pyspark Test](#a5)


--- 

## **Data** <a name="a1"></a> 

**Data의 경우에는 이전 포스트에서 다뤘었던 Covid-19의 기상 데이터를 기반으로 진행합니다.**  

![12312312](https://user-images.githubusercontent.com/69498804/116961186-9e637480-acdd-11eb-906f-9e340165dee1.JPG)

* **용량 : 약 51GB**
* **행 : 542,304,210**

<br/>


![2222](https://user-images.githubusercontent.com/69498804/116961225-bfc46080-acdd-11eb-930e-ec68574417e5.JPG)

* **데이터 형식 요약**

<br/>


---

## **Python Script** <a name="a2"></a> 

**위의 데이터에서 특정 그룹(나라, 날짜) 별로 MAX,MIN,AVG 값들의 평균 값을 구하는 스크립트** 

```
from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
sc = SparkContext('local')
spark = SparkSession(sc)
print(type(spark))



read_path = "gs://nasa_us/"
write_path = 'gs://proc_result/result/'


# def for columns cheange

# ------------------------------------------------------------------
def renameCols(df, old_columns, new_columns):
    for old_col,new_col in zip(old_columns,new_columns):
        df = df.withColumnRenamed(old_col,new_col)
    return df


# Old_columns
old_columns = ['avg(min_temperature_air_2m_f)',
                'avg(max_temperature_air_2m_f)',
                'avg(avg_temperature_air_2m_f)'
                ]

# New_columns
new_columns = ['temperature_air_min_avg',
                'temperature_air_max_avg',
                'temperature_air_avg_avg'
                ]
# --------------------------------------------
# ----------------------

# Read CSV from GCS
df = spark.read.csv(read_path, header=True, inferSchema=True)

# data transform
df = df.groupBy('country', 'date').agg({'min_temperature_air_2m_f' : 'avg', 'max_temperature_air_2m_f' : 'avg', 'avg_temperature_air_2m_f' : 'avg'})

df2 = renameCols(df, old_columns, new_columns)

# Write CSV to GCS
df2.coalesce(1).write.option("header", "true").mode("overwrite").csv(write_path)
```

* **간단 설명 : GCS에서 CSV Format의 Data를 읽고 ETL 작업 후 결과를 GCS에 저장**  
* **Bigquery Table Data를 csv화 시키고 GCS에 저장하는 방법은 [이전포스트](https://nasa1515.github.io/data/2021/04/28/DataProc.html)를 확인하세요**


<br/>


## **DataProc Job 생성** <a name="a3"></a> 


![333](https://user-images.githubusercontent.com/69498804/116962299-91945000-ace0-11eb-8e8f-20ea0f9f5b15.JPG)

* **위와 같이 DataProc - JOB -> 작업 제출로 JOB을 생성합니다.**  

<br/>

![44444](https://user-images.githubusercontent.com/69498804/116962386-c6a0a280-ace0-11eb-96f5-aaaad00c4588.JPG)

* **Cluster는 실행 할 Cluster를 지정합니다.**
* **작업 유형은 Pyspark를 선택합니다.** 
* **Python File의 경우 미리 GCS에 올려놓고 지정하면 됩니다.**  

<br/>

