---
layout: post
title: "[DATA] - Azure VM에 Apache Spark v3.0 설치 With Zeppelin"
author: nasa1515
categories: DATA
date: 2021-03-03 12:36
comments: true
cover: "/assets/1800-550.jpg"
tags: DATA
---



## **Azure VM에 Apache Spark v3.0 설치 With Zeppelin**


<br/>

**머리말**  

**저번 포스트에서 Apache Spark가 어떤 식으로 동작하는지? 어떤 함수가 있는지?**  
**간단하게 이론적으로만 알아봤습니다.**  
**아직 Spark에 대한 내용이 제대로 이해가 되지 않아 일단 구성부터 해보고**  
**실습을 하면서 다시 이해를 해보겠습니다.**  






  


 
---

**DATA 시리즈**

* **이론**


---



**목차**


- [Azure VM에 Spark StandAlone 구성](#a1)
- [Apache Spark](#a2)


--- 

<br/>

## **Azure VM에 Spark StandAlone 구성**   <a name="a1"></a>   

* **Spark StandAlone Cluster로 구성하는 포스트입니다.** 

<br/>

#### **환경구성 (Azure VM 기준으로 구성했습니다.)**  

  * **OS : CentOS Linux release 8.2.2004 (Core)**  
  * **cpu : 4 core**  
  * **RAM : 14GB**  


  <br/> 


### **1. Open JDK 설치 (root 계정 기반)**  

#### **Spark는 JVM 기반이기 때문에 JDK를 미리 설치해주어야 합니다.**  

```
[root@Spark-Standalone ~]# yum install -y java-1.8.0-openjdk-devel.x86_64
Last metadata expiration check: 0:28:13 ago on Thu 04 Mar 2021 07:30:20 AM UTC.
...
...
(중략)
Complete!
```

<br/>

#### **java version을 확인해보죠**  

```
[root@Spark-Standalone ~]# java -version
openjdk version "1.8.0_275"
OpenJDK Runtime Environment (build 1.8.0_275-b01)
OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode)
```

<br/>


### **2. Scala 설치 (root 계정 기반)**  

#### **scala 2.12.12 다운로드** 

```
[root@Spark-Standalone ~]# wget https://downloads.lightbend.com/scala/2.12.12/scala-2.12.12.tgz
```

<br/>

#### **/home/"계정"으로 파일 이동 후 압축 해제**

```
[root@Spark-Standalone ~]# mv scala-2.12.12.tgz /home/nasa1515/
[root@Spark-Standalone ~]# cd /home/nasa1515/
[root@Spark-Standalone nasa1515]# tar xvfz scala-2.12.12.tgz
scala-2.12.12/
```

<br/>

#### **사용자 설정** 

```
[root@Spark-Standalone nasa1515]# mv scala-2.12.12 scala
[root@Spark-Standalone nasa1515]# chown -R nasa1515:nasa1515 scala
```


<br/>

#### **환경 변수 설정 - ROOT -> nasa1515로 계정전환** 

#### **사용자 전환**
```
[root@Spark-Standalone nasa1515]# su nasa1515

```

#### **.bashrc 파일에 다음 내용 설정 후 적용** 

```
[nasa1515@Spark-Standalone ~]$ echo export PATH='$PATH':/home/nasa1515/scala/bin > ~/.bashrc
[nasa1515@Spark-Standalone ~]$ source ~/.bashrc
```

<br/>

#### **Scala 설치 확인** 

```
[nasa1515@Spark-Standalone ~]$ scala -version
Scala code runner version 2.12.12 -- Copyright 2002-2020, LAMP/EPFL and Lightbend, Inc.
```

<br/>



### **3. Apache Spark 설치 (root 계정 기반)**  

#### **Spark 3.1.1 다운로드**  

```
[root@Spark-Standalone ~]# wget https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz
```

<br/>


#### **파일이동 및 압축 해제**  

```
[root@Spark-Standalone ~]# mv spark-3.1.1-bin-hadoop2.7.tgz /home/spark/
[root@Spark-Standalone ~]# cd /home/spark/
[root@Spark-Standalone spark]# tar xvfz spark-3.1.1-bin-hadoop2.7.tgz
[root@Spark-Standalone spark]# mv spark-3.1.1-bin-hadoop2.7 spark
```

<br/>

#### **사용자 계정 등록** 

```
[root@Spark-Standalone spark]# chown -R spark:spark spark
```

<br/>

#### **Spark 환경변수 등록 (spark 계정으로 전환 후 진행)**  


```
[spark@Spark-Standalone ~]$ echo export PATH='$PATH':/home/spark/spark/bin > ~/.bashrc
[spark@Spark-Standalone ~]$ source ~/.bashrc
```


<br/>

#### **Spark 설치 확인** 

```
[spark@Spark-Standalone ~]$ spark-shell
21/03/04 08:37:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://spark-standalone.internal.cloudapp.net:4040
Spark context available as 'sc' (master = local[*], app id = local-1614847063399).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.1
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_275)
Type in expressions to have them evaluated.
Type :help for more information.

scala>
```

<br/>



### **4. Zeppelin 설치 (root 계정 기반)**  

#### **Zeppelin 다운로드**  

```
[root@Spark-Standalone zeppelin]# wget https://downloads.apache.org/zeppelin/zeppelin-0.9.0-preview2/zeppelin-0.9.0-preview2-bin-all.tgz
```

<br/>


#### **압축해제 및 권한 변경**

```
[root@Spark-Standalone nasa1515]# tar xvfz spark-3.1.1-bin-hadoop2.7.tgz
[root@Spark-Standalone zeppelin]# mv zeppelin-0.9.0-preview2-bin-all zeppelin
[root@Spark-Standalone zeppelin]# chown -R zeppelin:zeppelin zeppelin
```

<br/>

#### **환경 변수 등록 (zeppelin 계정으로 전환 후 진행)**  

```
[zeppelin@Spark-Standalone ~]$ echo export PATH='$PATH':/home/zeppelin/zeppelin/bin > ~/.bashrc
[zeppelin@Spark-Standalone ~]$ source ~/.bashrc
```

<br/>

#### **Zeppelin 환경 설정**  

```
## JAVA의 위치를 확인 (환경변수 설정을 위해)

[root@Spark-Standalone conf]# which javac
/usr/bin/javac
[root@Spark-Standalone conf]# readlink -f /usr/bin/javac
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-1.el8_3.x86_64/bin/javac
[root@Spark-Standalone conf]#
```

<br/>

```
### /home/zeppelin/zeppelin/conf 위치에서 Conf 파일 설정

[root@Spark-Standalone conf]# pwd
/home/zeppelin/zeppelin/conf
[root@Spark-Standalone conf]# cp zeppelin-env.sh.template zeppelin-env.sh

### 설정 추가

[root@Spark-Standalone conf]# echo export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk > zeppelin-env.sh
[root@Spark-Standalone conf]# echo export SPARK_HOME=/home/spark/spark >> zeppelin-env.sh
```

<br/>

```
### Web 접속을 위한 설정


[root@Spark-Standalone conf]# cp zeppelin-site.xml.template zeppelin-site.xml



```

<br/>


#### **Local로 잡혀있는 IP를 변경 (Azure 기반이기에 PIP로 설정)**
```
...
...
<property>
  <name>zeppelin.server.addr</name>
  <value>127.0.0.1</value> -> 0.0.0.0으로 변경해서 테스트!
  <description>Server binding address</description>
</property>
...
...
```

<br/>

#### **Zeppelin Daemon 기동**  

```
[root@Spark-Standalone conf]# zeppelin-daemon.sh start
Log dir doesn't exist, create /home/zeppelin/zeppelin/logs
Pid dir doesn't exist, create /home/zeppelin/zeppelin/run
Zeppelin start                                             [  OK  ]
```
<br/>

#### **Zeppelin Web page 확인**

![캡처3333](https://user-images.githubusercontent.com/69498804/109940335-47f5ac00-7d15-11eb-9c86-04ded7a40090.JPG)

<br/>


----


### **Error 발생!!**

#### **pyspark로 실습을 진행해보려 사전 검사를 하던 도중 다음과 같은 Error가 발현했다!**  

```
org.apache.zeppelin.interpreter.InterpreterException: java.io.IOException: Fail to launch interpreter process:
Interpreter launch command:  /home/spark/spark/bin/spark-submit --class org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer --driver-class-path ":/home/zeppelin/zeppelin/interpreter/spark/*::/home/zeppelin/zeppelin/interpreter/zeppelin-interpreter-shaded-0.9.0-preview2.jar:/home/zeppelin/zeppelin/interpreter/spark/spark-interpreter-0.9.0-preview2.jar" --driver-java-options " -Dfile.encoding=UTF-8 -Dlog4j.configuration='file:///home/zeppelin/zeppelin/conf/log4j.properties' -Dlog4j.configurationFile='file:///home/zeppelin/zeppelin/conf/log4j2.properties' -Dzeppelin.log.file='/home/zeppelin/zeppelin/logs/zeppelin-interpreter-spark-shared_process-zeppelin-Spark-Standalone.log'" --conf spark\.driver\.cores\=1 --conf spark\.executor\.memory\=1g --conf spark\.executor\.cores\=1 --conf spark\.webui\.yarn\.useProxy\=false --conf spark\.app\.name\=Zeppelin --conf spark\.driver\.memory\=1g --conf spark\.master\=local\[\*\] /home/zeppelin/zeppelin/interpreter/spark/spark-interpreter-0.9.0-preview2.jar 10.0.0.4 37799 "spark-shared_process" :
/home/zeppelin/zeppelin/bin/interpreter.sh: line 299: /home/spark/spark/bin/spark-submit: Permission denied

	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:134)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:281)
	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:412)
	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:72)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:172)
	at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:130)
	at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:180)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Fail to launch interpreter process:
Interpreter launch command:  /home/spark/spark/bin/spark-submit --class org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer --driver-class-path ":/home/zeppelin/zeppelin/interpreter/spark/*::/home/zeppelin/zeppelin/interpreter/zeppelin-interpreter-shaded-0.9.0-preview2.jar:/home/zeppelin/zeppelin/interpreter/spark/spark-interpreter-0.9.0-preview2.jar" --driver-java-options " -Dfile.encoding=UTF-8 -Dlog4j.configuration='file:///home/zeppelin/zeppelin/conf/log4j.properties' -Dlog4j.configurationFile='file:///home/zeppelin/zeppelin/conf/log4j2.properties' -Dzeppelin.log.file='/home/zeppelin/zeppelin/logs/zeppelin-interpreter-spark-shared_process-zeppelin-Spark-Standalone.log'" --conf spark\.driver\.cores\=1 --conf spark\.executor\.memory\=1g --conf spark\.executor\.cores\=1 --conf spark\.webui\.yarn\.useProxy\=false --conf spark\.app\.name\=Zeppelin --conf spark\.driver\.memory\=1g --conf spark\.master\=local\[\*\] /home/zeppelin/zeppelin/interpreter/spark/spark-interpreter-0.9.0-preview2.jar 10.0.0.4 37799 "spark-shared_process" :
/home/zeppelin/zeppelin/bin/interpreter.sh: line 299: /home/spark/spark/bin/spark-submit: Permission denied

	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterManagedProcess.start(RemoteInterpreterManagedProcess.java:125)
	at org.apache.zeppelin.interpreter.ManagedInterpreterGroup.getOrCreateInterpreterProcess(ManagedInterpreterGroup.java:67)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getOrCreateInterpreterProcess(RemoteInterpreter.java:110)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:160)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:131)
	... 13 more
```


#### **해당 ERROR는 Zeppelin의 Interpreters에서 Spark를 Restart 시켜서 해결했습니다.**  

<br/>

#### **이제 막 실습을 진행해보려는 찰나 다른 에러가 발생..**


```
%pyspark

var = 'hello world'


org.apache.zeppelin.interpreter.InterpreterException: org.apache.zeppelin.interpreter.InterpreterException: org.apache.zeppelin.interpreter.InterpreterException: Fail to open SparkInterpreter
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:76)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:760)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:668)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:172)
	at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:130)
	at org.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:39)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.zeppelin.interpreter.InterpreterException: org.apache.zeppelin.interpreter.InterpreterException: Fail to open SparkInterpreter
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:76)
	at org.apache.zeppelin.interpreter.Interpreter.getInterpreterInTheSameSessionByClassName(Interpreter.java:355)
	at org.apache.zeppelin.interpreter.Interpreter.getInterpreterInTheSameSessionByClassName(Interpreter.java:366)
	at org.apache.zeppelin.spark.PySparkInterpreter.open(PySparkInterpreter.java:89)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)
	... 8 more
Caused by: org.apache.zeppelin.interpreter.InterpreterException: Fail to open SparkInterpreter
	at org.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:122)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)
	... 12 more
Caused by: java.lang.Exception: This is not officially supported spark version: 3.1.1
You can set zeppelin.spark.enableSupportedVersionCheck to false if you really want to try this version of spark.
	at org.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:112)
	... 13 more
```

<br/>

#### **우선 JDK에 관련된 에러인가 싶어 환경변수를 다시 설정해줬습니다.**  

```
[root@Spark-Standalone ~]# echo export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk > /etc/profile
[root@Spark-Standalone ~]# echo $JAVA_HOME
/usr/lib/jvm/java-1.8.0-openjdk
[root@Spark-Standalone ~]#
[root@Spark-Standalone ~]# javac -version
javac 1.8.0_275
```

<br/>

#### **안댑니다..**


<br/>

#### **StackOverFlow를 좀 둘러보니 Spark와 Zeppelin의 jar파일이 다르면 발생한다고 한다**  

```
[root@Spark-Standalone jars]# /usr/bin/cp -f /home/spark/spark/jars/* /home/zeppelin/zeppelin/lib
[root@Spark-Standalone jars]#
```
**싹다 복사했습니다** 


<br/>

#### **이후에 혹시 모르니 그냥 VM을 재부팅 시켰습니다.**  

```
### Azure CLI

PS /home/nasa1515> az vm restart -g Spark-Cluster --name Spark-Standalone
PS /home/nasa1515> 
```

<br/>


#### **Zeppelin이 안켜집니다..ㅠㅠ**  
#### **결국 VM을 밀어내고 예상되는 문제인 Spark의 버젼을 낮추고 다시 설치해보겠습니다..**  


<br/>

#### **각 툴 별 설치 및 설정 명령어**

**JAVA**

```
[root@Standalone ~]# yum install -y *java*openjdk*
[root@Standalone ~]# echo export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk > /etc/profile
```

**Spark**

```
[root@Standalone home]# useradd spark
[root@Standalone home]# cd /home/spark/
[root@Standalone spark]# wget https://downloads.apache.org/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz
[root@Standalone spark]# tar xvfz spark-3.0.2-bin-hadoop2.7.tgz
[root@Standalone spark]# mv spark-3.0.2-bin-hadoop2.7 spark
[root@Standalone spark]# chown -R spark:spark spark
[root@Standalone spark]# chmod -R 777  spark
```

<br/>

**Zeppelin** 

```
[root@Standalone home]# useradd zeppelin
[root@Standalone home]# cd /home/zeppelin/
[root@Standalone zeppelin]# wget https://downloads.apache.org/zeppelin/zeppelin-0.9.0-preview2/zeppelin-0.9.0-preview2-bin-all.tgz
[root@Standalone zeppelin]# tar xvfz zeppelin-0.9.0-preview2-bin-all.tgz
[root@Standalone zeppelin]# mv zeppelin-0.9.0-preview2-bin-all zeppelin
[zeppelin@Standalone conf]$ cp zeppelin-env.sh.template zeppelin-env.sh
[zeppelin@Standalone conf]$ vim zeppelin-env.sh
[zeppelin@Standalone conf]$ echo export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk > zeppelin-env.sh
[zeppelin@Standalone conf]$ echo export SPARK_HOME=/home/spark/spark >> zeppelin-env.sh
[zeppelin@Standalone conf]$ cp zeppelin-site.xml.template zeppelin-site.xml
[zeppelin@Standalone conf]$
```

<br/>


**됐습니다ㅠㅠ**

![캡처121](https://user-images.githubusercontent.com/69498804/110066882-d3bf1500-7db5-11eb-901c-b2b00c1533cf.JPG)

<br/>

**정확한 원인을 알아낸거는 두가지였습니다.** 

* **1. zeppelin의 zeppelin-env.sh 설정에 JAVA_HOME 설정이 없었다.**  
* **2. Spark의 버전 호환이 Zeppelin과 맞지 않았다..**  

