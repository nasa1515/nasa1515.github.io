---
layout: post
title: "[DATA] - Azure VM에 Apache Spark v3.0 설치 With Zeppelin"
author: nasa1515
categories: DATA
date: 2021-03-03 12:36
comments: true
cover: "/assets/1800-550.jpg"
tags: DATA
---



## **Azure VM에 Apache Spark v3.0 설치 With Zeppelin**


<br/>

**머리말**  

**저번 포스트에서 Apache Spark가 어떤 식으로 동작하는지? 어떤 함수가 있는지?**  
**간단하게 이론적으로만 알아봤습니다.**  
**아직 Spark에 대한 내용이 제대로 이해가 되지 않아 일단 구성부터 해보고**  
**실습을 하면서 다시 이해를 해보겠습니다.**  






  


 
---

**DATA 시리즈**

* **이론**


---



**목차**


- [Azure VM에 Spark StandAlone 구성](#a1)
- [Apache Spark](#a2)


--- 

<br/>

## **Azure VM에 Spark StandAlone 구성**   <a name="a1"></a>   

* **Spark StandAlone Cluster로 구성하는 포스트입니다.** 

<br/>

#### **환경구성 (Azure VM 기준으로 구성했습니다.)**  

  * **OS : CentOS Linux release 8.2.2004 (Core)**  
  * **cpu : 4 core**  
  * **RAM : 14GB**  


  <br/> 


### **1. Open JDK 설치 (root 계정 기반)**  

#### **Spark는 JVM 기반이기 때문에 JDK를 미리 설치해주어야 합니다.**  

```
[root@Spark-Standalone ~]# yum install -y java-1.8.0-openjdk-devel.x86_64
Last metadata expiration check: 0:28:13 ago on Thu 04 Mar 2021 07:30:20 AM UTC.
...
...
(중략)
Complete!
```

<br/>

#### **java version을 확인해보죠**  

```
[root@Spark-Standalone ~]# java -version
openjdk version "1.8.0_275"
OpenJDK Runtime Environment (build 1.8.0_275-b01)
OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode)
```

<br/>


### **2. Scala 설치 (root 계정 기반)**  

#### **scala 2.12.12 다운로드** 

```
[root@Spark-Standalone ~]# wget https://downloads.lightbend.com/scala/2.12.12/scala-2.12.12.tgz
```

<br/>

#### **/home/"계정"으로 파일 이동 후 압축 해제**

```
[root@Spark-Standalone ~]# mv scala-2.12.12.tgz /home/nasa1515/
[root@Spark-Standalone ~]# cd /home/nasa1515/
[root@Spark-Standalone nasa1515]# tar xvfz scala-2.12.12.tgz
scala-2.12.12/
```

<br/>

#### **사용자 설정** 

```
[root@Spark-Standalone nasa1515]# mv scala-2.12.12 scala
[root@Spark-Standalone nasa1515]# chown -R nasa1515:nasa1515 scala
```


<br/>

#### **환경 변수 설정 - ROOT -> nasa1515로 계정전환** 

#### **사용자 전환**
```
[root@Spark-Standalone nasa1515]# su nasa1515

```

#### **.bashrc 파일에 다음 내용 설정 후 적용** 

```
[nasa1515@Spark-Standalone ~]$ echo export PATH='$PATH':/home/nasa1515/scala/bin > ~/.bashrc
[nasa1515@Spark-Standalone ~]$ source ~/.bashrc
```

<br/>

#### **Scala 설치 확인** 

```
[nasa1515@Spark-Standalone ~]$ scala -version
Scala code runner version 2.12.12 -- Copyright 2002-2020, LAMP/EPFL and Lightbend, Inc.
```

<br/>



### **3. Apache Spark 설치 (root 계정 기반)**  

#### **Spark 3.0.2 다운로드**  

```
[root@Spark-Standalone ~]# wget https://downloads.apache.org/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz
```

<br/>


#### **압축해제 및 권한 설정**  

```
[root@Standalone home]# useradd spark
[root@Standalone home]# cd /home/spark/
[root@Standalone spark]# tar xvfz spark-3.0.2-bin-hadoop2.7.tgz
[root@Standalone spark]# mv spark-3.0.2-bin-hadoop2.7 spark
[root@Standalone spark]# chown -R spark:spark spark
[root@Standalone spark]# chmod -R 777  spark
```

<br/>

#### **Spark 환경변수 등록 (spark 계정으로 전환 후 진행)**  


```
[spark@Spark-Standalone ~]$ echo export PATH='$PATH':/home/spark/spark/bin > ~/.bashrc
[spark@Spark-Standalone ~]$ source ~/.bashrc
```


<br/>

#### **Spark 설치 확인** 

```
[spark@Spark-Standalone ~]$ spark-shell
21/03/04 08:37:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://spark-standalone.internal.cloudapp.net:4040
Spark context available as 'sc' (master = local[*], app id = local-1614847063399).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.1
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_275)
Type in expressions to have them evaluated.
Type :help for more information.

scala>
```


<br/>

#### **Spark Config 설정** 

```
## Spark_HOME의 conf 디렉토리에서 수행

[root@Standalone conf]# cp spark-env.sh.template spark-env.sh
[root@Standalone conf]# vim spark-env.sh


## nasa setting

export SPARK_WORKER_INSTANCES=3 [worker node에 대한 설정]
```


#### **Spark Master 프로세스 실행**

```
[root@Standalone sbin]# sh start-master.sh
starting org.apache.spark.deploy.master.Master, logging to /home/spark/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-Standalone.out
```

#### **MASTER WEB 확인 (Cloud라면 Inbound 설정 필요)**

![캡처3121](https://user-images.githubusercontent.com/69498804/110070522-86df3c80-7dbd-11eb-82cd-85a45eb4eeb2.JPG)

* **Worker Node 설정에 필요한 url 복사**  
    **spark://Standalone.44p1qlnthrou1ezmysbcbmontc.syx.internal.cloudapp.net:7077**


<br/>

#### **Worker Node 프로세스 실행**

**아까 env.sh 설정에 맞게 3개의 Worker 가 생성됩니다.** 
```
[root@Standalone sbin]# sh start-slave.sh spark://Standalone.44p1qlnthrou1ezmysbcbmontc.syx.internal.cloudapp.net:7077 -m 2g -c 1
starting org.apache.spark.deploy.worker.Worker, logging to /home/spark/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-Standalone.out
starting org.apache.spark.deploy.worker.Worker, logging to /home/spark/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-2-Standalone.out
starting org.apache.spark.deploy.worker.Worker, logging to /home/spark/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-3-Standalone.out
```

<br/>

#### **Master WEB에서 Worker 등록 확인**

![44444](https://user-images.githubusercontent.com/69498804/110070820-23a1da00-7dbe-11eb-8014-304fd2583ef8.JPG)


<br/>


### **3. Zeppelin 설치 (root 계정 기반)**  

#### **Zeppelin 다운로드**  

```
[root@Spark-Standalone zeppelin]# wget https://downloads.apache.org/zeppelin/zeppelin-0.9.0-preview2/zeppelin-0.9.0-preview2-bin-all.tgz
```

<br/>


#### **압축해제 및 권한 변경**

```
[root@Standalone home]# useradd zeppelin
[root@Standalone home]# cd /home/zeppelin/
[root@Standalone zeppelin]# tar xvfz zeppelin-0.9.0-preview2-bin-all.tgz
[root@Standalone zeppelin]# mv zeppelin-0.9.0-preview2-bin-all zeppelin
[root@Standalone zeppelin]# chown -R zeppelin:zeppelin /home/zeppelin
[root@Standalone zeppelin]# chmod -R 777 /home/zeppelin
```

<br/>

#### **환경 변수 등록 (zeppelin 계정으로 전환 후 진행)**  

```
[zeppelin@Standalone conf]$ cp zeppelin-env.sh.template zeppelin-env.sh
[zeppelin@Standalone conf]$ vim zeppelin-env.sh
[zeppelin@Standalone conf]$ echo export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk > zeppelin-env.sh
[zeppelin@Standalone conf]$ echo export SPARK_HOME=/home/spark/spark >> zeppelin-env.sh
[zeppelin@Standalone conf]$ cp zeppelin-site.xml.template zeppelin-site.xml
[zeppelin@Standalone conf]$


[zeppelin@Spark-Standalone ~]$ echo export PATH='$PATH':/home/zeppelin/zeppelin/bin > ~/.bashrc
[zeppelin@Spark-Standalone ~]$ source ~/.bashrc
```

<br/>

#### **Zeppelin 환경 설정**  

```
## JAVA의 위치를 확인 (환경변수 설정을 위해)

[root@Spark-Standalone conf]# which javac
/usr/bin/javac
[root@Spark-Standalone conf]# readlink -f /usr/bin/javac
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-1.el8_3.x86_64/bin/javac
[root@Spark-Standalone conf]#
```

<br/>

```
### /home/zeppelin/zeppelin/conf 위치에서 Conf 파일 설정

[root@Spark-Standalone conf]# pwd
/home/zeppelin/zeppelin/conf
[root@Spark-Standalone conf]# cp zeppelin-env.sh.template zeppelin-env.sh

### 설정 추가

[root@Spark-Standalone conf]# echo export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk > zeppelin-env.sh
[root@Spark-Standalone conf]# echo export SPARK_HOME=/home/spark/spark >> zeppelin-env.sh
```

<br/>

```
### Web 접속을 위한 설정


[root@Spark-Standalone conf]# cp zeppelin-site.xml.template zeppelin-site.xml



```

<br/>


#### **Local로 잡혀있는 IP를 변경 (Azure 기반이기에 PIP로 설정)**
```
...
...
<property>
  <name>zeppelin.server.addr</name>
  <value>127.0.0.1</value> -> 0.0.0.0으로 변경해서 테스트!
  <description>Server binding address</description>
</property>
...
...
```

<br/>

#### **Zeppelin Daemon 기동**  

```
[root@Spark-Standalone conf]# zeppelin-daemon.sh start
Log dir doesn't exist, create /home/zeppelin/zeppelin/logs
Pid dir doesn't exist, create /home/zeppelin/zeppelin/run
Zeppelin start                                             [  OK  ]
```
<br/>

#### **Zeppelin Web page 확인**

![캡처3333](https://user-images.githubusercontent.com/69498804/109940335-47f5ac00-7d15-11eb-9c86-04ded7a40090.JPG)

<br/>


----


#### **우선 JDK에 관련된 에러인가 싶어 환경변수를 다시 설정해줬습니다.**  

```
[root@Spark-Standalone ~]# echo export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk > /etc/profile
[root@Spark-Standalone ~]# echo $JAVA_HOME
/usr/lib/jvm/java-1.8.0-openjdk
[root@Spark-Standalone ~]#
[root@Spark-Standalone ~]# javac -version
javac 1.8.0_275
```

<br/>

#### **안댑니다..**


<br/>

#### **StackOverFlow를 좀 둘러보니 Spark와 Zeppelin의 jar파일이 다르면 발생한다고 한다**  

```
[root@Spark-Standalone jars]# /usr/bin/cp -f /home/spark/spark/jars/* /home/zeppelin/zeppelin/lib
[root@Spark-Standalone jars]#
```
**싹다 복사했습니다** 


<br/>

#### **이후에 혹시 모르니 그냥 VM을 재부팅 시켰습니다.**  

```
### Azure CLI

PS /home/nasa1515> az vm restart -g Spark-Cluster --name Spark-Standalone
PS /home/nasa1515> 
```

<br/>


#### **Zeppelin이 안켜집니다..ㅠㅠ**  
#### **결국 VM을 밀어내고 예상되는 문제인 Spark의 버젼을 낮추고 다시 설치해보겠습니다..**  


<br/>

#### **각 툴 별 설치 및 설정 명령어**

**JAVA**

```
[root@Standalone ~]# yum install -y *java*openjdk*
[root@Standalone ~]# echo export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk > /etc/profile
```

**Spark**

```
[root@Standalone home]# useradd spark
[root@Standalone home]# cd /home/spark/
[root@Standalone spark]# wget https://downloads.apache.org/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz
[root@Standalone spark]# tar xvfz spark-3.0.2-bin-hadoop2.7.tgz
[root@Standalone spark]# mv spark-3.0.2-bin-hadoop2.7 spark
[root@Standalone spark]# chown -R spark:spark spark
[root@Standalone spark]# chmod -R 777  spark
```

<br/>

**Zeppelin** 

```
[root@Standalone home]# useradd zeppelin
[root@Standalone home]# cd /home/zeppelin/
[root@Standalone zeppelin]# wget https://downloads.apache.org/zeppelin/zeppelin-0.9.0-preview2/zeppelin-0.9.0-preview2-bin-all.tgz
[root@Standalone zeppelin]# tar xvfz zeppelin-0.9.0-preview2-bin-all.tgz
[root@Standalone zeppelin]# mv zeppelin-0.9.0-preview2-bin-all zeppelin
[zeppelin@Standalone conf]$ cp zeppelin-env.sh.template zeppelin-env.sh
[zeppelin@Standalone conf]$ vim zeppelin-env.sh
[zeppelin@Standalone conf]$ echo export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk > zeppelin-env.sh
[zeppelin@Standalone conf]$ echo export SPARK_HOME=/home/spark/spark >> zeppelin-env.sh
[zeppelin@Standalone conf]$ cp zeppelin-site.xml.template zeppelin-site.xml
[zeppelin@Standalone conf]$
```

<br/>
