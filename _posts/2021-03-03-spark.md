---
layout: post
title: "[DATA] - Apache Spark 구성"
author: nasa1515
categories: DATA
date: 2021-03-03 12:36
comments: true
cover: "/assets/1800-550.jpg"
tags: DATA
---



## **Apache Spark 구성**


<br/>

**머리말**  

**이번에는 데이터의 가장 기초적인 오픈소스인 Apache Spark에 대한 내용 정리입니다.**  
**아무것도 모르는 생짜 초보이기 때문에 사소한 것 까지 놓치지않고 정리했습니다.**  




  


 
---

**DATA 시리즈**

* **이론**


---



**목차**


- [Apache Spark?](#a1)
- [-](#a2)
- [-](#a3)


--- 

<br/>

## **Apache Spark?**   <a name="a1"></a>   


![캡처1](https://user-images.githubusercontent.com/69498804/109732527-b3018e80-7c00-11eb-8fc9-53e9618bfac5.JPG)

**주워들었었던 지식으로는 데이터 시장은 오픈소스인 Hadoop과 Apache가 경쟁하며 성장하고 있다고 알고 있다**  
**그런데 또 다른 글들을 보니 이미 업계에서는 두 오픈소스를 동시에 사용한다고도 한다.**
**경쟁하는 관계인데 또 상생을 하고 있다는게 무슨소리지?**   
**다시 한번 찾아보니 각각의 툴의 용도에 대해서 알지 못했던 나의 오착이었다.**  

<br/>

**내가 이해한 두 앱의 용도를 간단하게 설명해보면**  
**우선 두 툴은 빅데이터 처리 플랫폼, 프레임워크라는 공통점을 가지고 있지만**  
**Hadoop은 분산 데이터 Infrastructure를 주로, 대량의 데이터를 Server Cluster의 복수 노드들에 분산시키는 역할을 한다.**  
**이를 통해 데이터 처리를 위한 필요한 하드웨어의 비용부담을 줄여준다.**   
**반면에 Spark는 분산 데이터 컬렉션 상부에서 동작하는 ``데이터 프로세싱 툴``로 분산형 스토리지의 역할은 수행하지 않는다고 한다.**  
**대충 이 대목에서 왜 두 오픈소스를 상생하면서 쓰는지 감이오기 시작한다.**  

<br/>
 
**Hadoop은 HDFS(Hadoop Database filesystem)을 사용하며 맵리듀스를 핵심 구성 요소로 제공한다. 따라서 Spark가 없어도 된다.**   
**반대로 Spark도 HDFS가 아닌 AWS,GCP,Azure 등과 융합될 수 있기에 Hadoop이 없어도 된다.**  

**또한 두 툴의 확실한 차이는 속도에서 확인이 가능하다.** 

**일반적인 상황에서 Hadoop보다 스파크의 속도가 월등히 빠르다고 한다.**  
**이는 데이터 프로세싱 절차의 차이라고 한다**  
**Hadoop은 MapReduce를 사용하기 때문이고, Spark는 DataSet 전체를 한번에 다루기 때문이다.**  

<br/>

* **Hadoop의 Mapreduce WorkFlow**  

    **Input -> Splitting -> Mapping -> Shuffling -> Reducing -> Final Result**

    ![99F6AA445B5975A320](https://user-images.githubusercontent.com/69498804/109735558-4ee1c900-7c06-11eb-85aa-5fd05dc011f1.jpg)


    **INPUT : 먼저 클러스터에서 데이터를 읽고**  
    **(클라이언트->네임노드->클라이언트->데이터 노드-> 마스터(Job Tracker))**  
    **태스크 단위로 쪼개어 Tasketracker(worker)에 배정한 다음**  
    **Map 단계를 수행한 후, 중간 결과물을 로컬 디스크에 저장을 한다.**  
    **그리고 그 결과물을 다시 combine, partioning을 거쳐 나온 2차 중간 결과물을 디스크에 분할 저장한다.**  
    **그리고 최종적으로 shuffling을 통해 reduce 작업에 할당된 후**  
    **reduce 작업을 거쳐 최종적으로 나온 결과물이 HDFS에 저장된다".**



**이에 반해, 스파크는 모든 데이터 운영을 메모리 내에서 실시간에 가깝게 처리할 수 있다(인메모리).**  
**데이터를 읽고, 처리 분석을 거친 결과물을 클러스터에 입력하는 전 과정이 동시에 진행되는 것이다.**  
**배치 프로세싱 경우에 스파크가 10배 빠르고, 인 메모리 애널리틱스의 경우, 속도 차이가 100배에 이른다고 알려져있다.**   

<br/>

**나는 여기서 왜 Spark가 더 좋은데 Hadoop을 쓰지? 라는 의문이 들었다.**  
**그러나 Data 운영, 리포팅 요구의 대부분이 정적인 것들이고, 시간의 여유가 있다면 Mapreduce의 방식을 채택한다고 한다.**  
**다만 Spark가 필수적으로 필요할 때가 있는데 이는 비즈니스 공장의 센서 등 실시간으로 수집되는 스트리밍 데이터를 처리하거나, ML 알고리즘과 같이 APP의 복합적인 운영을 할때라고 한다.**  
