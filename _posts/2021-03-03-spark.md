---
layout: post
title: "[DATA] - Apache Spark 구성"
author: nasa1515
categories: DATA
date: 2021-03-03 12:36
comments: true
cover: "/assets/1800-550.jpg"
tags: DATA
---



## **Apache Spark 구성**


<br/>

**머리말**  

**이번에는 데이터의 가장 기초적인 오픈소스인 Apache Spark에 대한 내용 정리입니다.**  
**아무것도 모르는 생짜 초보이기 때문에 사소한 것 까지 놓치지않고 정리했습니다.**  




  


 
---

**DATA 시리즈**

* **이론**


---



**목차**


- [Apache Spark? Hadoop?](#a1)
- [Apache Spark](#a2)
- [-](#a3)


--- 

<br/>

## **Apache Spark? Hadoop?**   <a name="a1"></a>   


![캡처1](https://user-images.githubusercontent.com/69498804/109732527-b3018e80-7c00-11eb-8fc9-53e9618bfac5.JPG)

**주워들은 말로는 데이터 시장은 오픈소스인 Hadoop과 Apache가 경쟁하며 성장하고 있다고 알고 있다**  
**그런데 또 다른 글들을 보니 이미 업계에서는 두 오픈소스를 동시에 사용한다고도 한다.**   
**경쟁하는 관계인데 또 상생을 하고 있다는게 무슨소리지?**   
**다시 한번 찾아보니 각각의 툴의 용도에 대해서 알지 못했던 나의 오착이었다.**  

<br/>

**내가 이해한 두 앱의 용도를 간단하게 설명해보면**  
**우선 두 툴은 빅데이터 처리 플랫폼, 프레임워크라는 공통점을 가지고 있지만** 

* **Hadoop**  
    **분산 데이터 Infrastructure를 주로 하며,**  
    **대량의 데이터를 Server Cluster 내 복수 노드들에 분산시키는 역할을 한다.**  
    **이를 통해 데이터 처리를 위한 필요한 하드웨어의 비용부담을 줄여준다.**   

**반면에 Spark는 분산 데이터 컬렉션 상부에서 동작하는 ``데이터 프로세싱 툴``로**  
**분산형 스토리지의 역할은 수행하지 않는다고 한다.**    
**대충 이 대목에서 왜 두 오픈소스를 상생하면서 쓰는지 감이오기 시작했다.**  

<br/>
 
**``Hadoop``은 HDFS(Hadoop Database filesystem)을 사용하며 맵리듀스를 핵심 구성 요소로 제공한다.**  
**따라서 Spark가 없어도 된다.**   
**반대로 ``Spark``도 HDFS가 아닌 AWS,GCP,Azure 등과 융합될 수 있기에 Hadoop이 없어도 된다.**    
**그러나 Haddop과 Spark를 같이 사용할때가 가장 적합하다고 한다.**  


<br/>

**두 툴의 확실한 차이는 속도에서 확인이 가능하다.** 

**일반적인 상황에서 Hadoop보다 스파크의 속도가 월등히 빠르다고 한다.**  
**이유는 데이터 프로세싱 절차의 차이 때문이라고 한다.**  
**Hadoop은 MapReduce를 사용하기 때문이고, Spark는 DataSet 전체를 한번에 다루기 때문에....**  

<br/>

* **Hadoop의 Mapreduce WorkFlow**  

    **Input -> Splitting -> Mapping -> Shuffling -> Reducing -> Final Result**

    ![99F6AA445B5975A320](https://user-images.githubusercontent.com/69498804/109735558-4ee1c900-7c06-11eb-85aa-5fd05dc011f1.jpg)


    ***INPUT : 먼저 클러스터에서 데이터를 읽고***  
    ***클라이언트->네임노드->클라이언트->데이터 노드-> 마스터(Job Tracker)***  
    ***태스크 단위로 쪼개어 Tasketracker(worker)에 배정하고***  
    ***Map 단계를 수행한 후, 중간 결과물을 로컬 디스크에 저장을 한다.***  
    ***그리고 그 결과물을 다시 combine, partioning을 거쳐 나온 2차 중간 결과물을 디스크에 분할 저장한다.***  
    ***그리고 최종적으로 shuffling을 통해 reduce 작업에 할당된 후***  
    ***reduce 작업을 거쳐 최종적으로 나온 결과물이 HDFS에 저장된다".***

<br/>


**이에 반해, 스파크는 모든 데이터 운영을 메모리 내에서 실시간에 가깝게 처리할 수 있다(인메모리).**  
**데이터를 읽고, 처리 분석을 거친 결과물을 클러스터에 입력하는 전 과정이 동시에 진행되는 것이다.**  
**배치 프로세싱 경우에 스파크가 10배 빠르고, 인 메모리 Analytics의 경우, 100배 빠르다고 알려져있다.**   

<br/>

**나는 여기서 왜 Spark가 더 좋은데 Hadoop을 쓰지? 라는 의문이 들었다.**  
**그러나 대부분의 Data 운영, 리포팅 요구의 대부분이 정적인 것들이고**  
**시간의 여유가 있다면 Mapreduce의 방식을 채택한다고 한다.**  
**다만 Spark가 필수적으로 필요할 때가 있는데 이는 비즈니스 공장의 센서 등 실시간으로 수집되는 스트리밍 데이터를 처리하거나, ML 알고리즘과 같이 APP의 복합적인 운영을 할때라고 한다.**    
**그리고 애초에 Hadoop만 사용하다가 위와 같이 실시간 적인 데이터 처리를 위해서 도입한 것이 Spark라서 그냥 두 툴을 같이 쓰는게 최적이라고 한다.**  

<br/>

---



## **Apache Spark**   <a name="a2"></a>  

**그럼 간단하게 데이터 플랫폼 2개의 툴에 대해서 설명했으니**  
**오늘 포스트의 주제인 Spark에 대한 내용으로 돌아와보자** 

* ### **Spark의 구성 요소**

    ![components_of_spark](https://user-images.githubusercontent.com/69498804/109738566-61aacc80-7c0b-11eb-9c66-5f50dff0e63b.jpg)


    **Spark는 다음 그림과 같은 구성 요소를 가지고 있습니다.**

### **Apache Spark Core**

* **Spark job과 다른 Spark 컴포넌트에 필요한 기본 기능을 제공합니다.**  
* **주로 분산 데이터 컬렉션(DataSet)을 추상화한 객체 RDD로 다양한 연산, 변환 메소드를 제공합니다.**
* **HDFS, GlusterFS, S3등 여러 Filsystem에 접근이 가능합니다.**  
* **공유 변수, 누적 변수를 사용해 컴퓨팅 노드 간 정보를 공유합니다.**  
* **Spark core에는 네트워킹, 보안, 스케쥴링 및 데이터 셔플링 등 기본 기능을 제공합니다.**  



### **Spark SQL**

    **Spark SQL은 Spark Core의 상단에 있는 컴포넌트로, SchemaRDD**  
    **라는 새로운 데이터 추상화를 도입하는 것으로, 구조화 및 반구조화 데이터를 지원한다.**

### **Spark Streaming**

    **Spark Streaming은 Spark Core의 빠른 스케줄링 기능을 활용하여 스트리밍 분석을 수행한다.**  
    **미니 베이스로 데이터를 수집해, 그러한 데이터 베이스에 대해 RDD(Resilient Distributed Datasets) 변환을 실시한다.**

### **MLlib**

    **MLlib는 분산된 메모리 기반 스파크 아키텍처 때문에 스파크 위에 분산된 기계 학습 프레임워크이다.**  

### **GraphX**

    **GraphX는 스파크 상단에 분산된 그래프 처리 프레임워크다.**   **(Pregel 추상화 API를 이용하여 사용자 정의 그래프를 모델링할 수 있는 그래프 연산을 표현하기 위한 API를 제공한다.**  
    **또한 이 추상화를 위해 최적화된 런타임을 제공한다.**

